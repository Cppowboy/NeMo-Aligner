trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: False # logger provided by exp_manager
  precision: bf16 # 16, 32, or bf16
  use_distributed_sampler: False

  deep_search:
    max_epochs: 1
    max_steps: -1
    val_check_interval: 1

pretrained_checkpoint:
  restore_from_path: null
 
model:
  mcts:
    C: 2 # weight for the UCB piror term
    num_searches: 800  # number of MCTS searches
    num_self_play_iterations: 2 # number of self play iterations
    self_play_batch_size: 2 # batch size for each dp worker to handle
    temperature: 0.2  # use low temperature for more greedy search
    dirichlet_epsilon: 0.0  # weight for dirichelt noise added to the root state, turn off the dirichlet noise by setting this to 0
    dirichlet_alpha: 0.3 # parameter for dirichlet noise, the piror probability of the action happens 
    max_depth: 250  # maxium depth of the search tree

    top_k: 50
    end_strings: ["<extra_id_1>"]  # generation will stop when one of these tokens is generated
    micro_batch_size: 16

  inference:

    sampling_params:
      greedy: True # Whether or not to use sampling ; use greedy decoding otherwise
      top_k: ${model.mcts.top_k}  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
      top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
      temperature: 1.0 # sampling temperature
      repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
      add_BOS: False # add the bos token at the begining of the prompt
      all_probs: False  # whether return the log prob for all the tokens in vocab
      compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
      end_strings: ${model.mcts.end_strings}

    length_params:
      max_length: 100
      min_length: 0

  train:
    value_weight: 1 # weight of the value portion of the loss
    policy_weight: 1 # weight of the policy portion of the loss

  micro_batch_size: 1
  global_batch_size: 64

  mcore_gpt: True
  share_embeddings_and_output_weights: False

  # reward_model_type: binary_ranking # ["binary_ranking, "regression"]
  regression:
    num_attributes: 1 # dimension of regression head
    merge_attributes: False # whether to merge multiple attributes into a scalar
    attribute_weights: null # apply these weights to each attributes when merging them into a scalar
    loss_mask_val: -100 #  mask dimensions with this value when calculating MSE loss

  output_sequence: True  # Whether to output a single scalar or a sequence of scalars.
  use_avg_pool: False  # Whether to use avg pool to sum across the sequence dim in reward model
  force_head_dtype: float32  # enforce specific dtype for the final projection in the model head
  megatron_amp_O2: True
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  pipeline_model_parallel_split_rank: 0 # used for encoder and decoder model (0 for others)
  encoder_seq_length: 4096
  max_position_embeddings: ${model.encoder_seq_length}

  # parameters for value output
  value:
    max_position_embeddings: ${model.encoder_seq_length}
    seed: 1234
    num_layers: 2 # two layers
    tensor_model_parallel_size: ${model.tensor_model_parallel_size}

  # miscellaneous
  seed: 1234

  optim:
    name: sgd

    sched:
      name: CosineAnnealing
      warmup_steps: 10
      constant_steps: 1000
      min_lr: 9e-7
# optim:
# name: distributed_fused_adam
# bucket_cap_mb: 200
# overlap_grad_sync: False
# contiguous_grad_buffer: True
# lr: 9e-6
# weight_decay: 0.1
# betas:
# - 0.9
# - 0.98
# sched:
# name: CosineAnnealing
# warmup_steps: 10
# constant_steps: 1000
# min_lr: 9e-7

  data:
    data_impl: jsonl
    splits_string: null
    seq_length: ${model.encoder_seq_length}
    skip_warmup: True
    num_workers: 2
    dataloader_type: single # cyclic
    reset_position_ids: False # Reset position ids after end-of-document token
    reset_attention_mask: False # Reset attention mask after end-of-document token
    eod_mask_loss: False # Mask loss for the end of document tokens
    index_mapping_dir: null # path to save index mapping .npy files, by default will save in the same location as data_prefix
    data_prefix: null

  precision: ${trainer.precision}

  # define fields from the base model's config that should be ignored when merging with this config.
  overwrite_base_config:
      data:
        data_prefix: True

exp_manager:
  explicit_log_dir: /results
  exp_dir: null
  name: megatron_gpt_ppo_hybrid
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: nemo_aligner_mcts
    name: gpt_10b
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: False
  checkpoint_callback_params:
    monitor: val_global_rewards
    save_top_k: 1
    mode: max
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    save_nemo_on_train_end: True # not recommended when training large models on clusters with short time limits
    filename: 'megatron_gpt-{step}-{consumed_samples}-{ppo_optimization_step}-{epoch}-{val_global_rewards:.3f}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}
