trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: False # logger provided by exp_manager
  precision: 16 # 16, 32, or bf16
  use_distributed_sampler: False
  

tensor_model_parallel_size: -1
pipeline_model_parallel_size: -1
pipeline_model_parallel_split_rank: -1 # used for encoder and decoder model (0 for others)
megatron_amp_O2: True  # Enable O2-level automatic mixed precision to save memory
checkpoint_dir: null # checkpoint file dir. This is used to load the PTL checkpoint generated during the GPT training
checkpoint_name: null # PTL checkpoint file name, only used for PTL checkpoint loading
nemo_file_path: null  # GPT nemo file path
tokenizer: # only used for PTL checkpoint loading
  library: sentencepiece
  type: null
  model: /dataset/models/llama2-13b/llama-tokenizer.model
  vocab_file: null
  merge_file: null
  tokenizer_model: /dataset/models/llama2-13b/llama-tokenizer.model
  sentencepiece_legacy: False
